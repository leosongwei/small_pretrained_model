{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[320]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glm3_tokenizer\n",
    "import torch\n",
    "\n",
    "from typing import Any, Union, List\n",
    "\n",
    "class CMMLUTokenizer:\n",
    "    def __init__(self, real_tokenizer: glm3_tokenizer.GLM3Tokenizer) -> None:\n",
    "        self._tokenizer = real_tokenizer\n",
    "\n",
    "    def encode(self, string: str, return_tensors=\"pt\"):\n",
    "        return self._tokenizer.encode_one_no_attn_no_special(string).tolist()\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]):\n",
    "        if isinstance(tokens, str):\n",
    "            tokens = [tokens]\n",
    "        \n",
    "        return [int(self._tokenizer.encode_one_no_attn_no_special(token)[0]) for token in tokens]\n",
    "    \n",
    "    def __call__(self, string, return_tensors=\"pt\") -> Any:\n",
    "        return self._tokenizer.encode(string, parse_special_tokens=False)\n",
    "\n",
    "_glm3_tokenizer = glm3_tokenizer.GLM3Tokenizer()\n",
    "cmmlu_tokenizer = CMMLUTokenizer(_glm3_tokenizer)\n",
    "cmmlu_tokenizer.convert_tokens_to_ids([\"A\"])\n",
    "# cmmlu_tokenizer([\"a b c\", \"1 2 3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_model\n",
    "import os\n",
    "import safetensors.torch\n",
    "import torch\n",
    "\n",
    "config = custom_model.CustomModelConfig(\n",
    "            vocab_size=_glm3_tokenizer.vocab_size(),\n",
    "            padding_token_id=_glm3_tokenizer.token_pad_id,\n",
    "            max_position_embeddings=4096,\n",
    "            hidden_size=704,\n",
    "            num_heads=16,\n",
    "            MLP_intermediate=5000,\n",
    "            num_layers=28,\n",
    "            attention_dropout=0.1,\n",
    "            dtype=torch.bfloat16,\n",
    "            training=True,\n",
    "            linear_imp = torch.nn.Linear\n",
    "        )\n",
    "\n",
    "model = custom_model.CustomLLamaModel(config).to(device='cuda')\n",
    "model_dir = r\"/mnt/sata2tb/leo/NLP/small_pretrained_model/saves_medium/checkpoint-751346922-751.35M\"\n",
    "model_path = os.path.join(model_dir, \"model.safetensor\")\n",
    "safetensors.torch.load_model(model, model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000],\n",
       "        [0.1000, 0.3000],\n",
       "        [0.5000, 0.2000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[.1, .2, .3, .4], [.1, .3, .3, .4], [.5, .2, .3, .4]])\n",
    "a[:, [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CMMLU.mp_utils\n",
    "from CMMLU.mp_utils import choices, format_example, gen_prompt, softmax\n",
    "# from CMMLU.hf_causal_model import eval\n",
    "from typing import Any\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "#def eval(model, tokenizer, subject, dev_df, test_df, num_few_shot, max_length, cot):\n",
    "# def eval(*args, **kwargs):\n",
    "#     print(repr(args))\n",
    "#     #print(repr(kwargs))\n",
    "#     for k, v in kwargs.items():\n",
    "#         print(f\"{k}: {type(v)}\")\n",
    "\n",
    "# model: <class 'NoneType'>\n",
    "# tokenizer: <class 'NoneType'>\n",
    "# subject: <class 'str'>\n",
    "# dev_df: <class 'pandas.core.frame.DataFrame'>\n",
    "# test_df: <class 'pandas.core.frame.DataFrame'>\n",
    "# num_few_shot: <class 'int'>\n",
    "# max_length: <class 'int'>\n",
    "# cot: <class 'bool'>\n",
    "def eval(\n",
    "        model: Any,\n",
    "        tokenizer: Any,\n",
    "        subject: str,\n",
    "        dev_df: DataFrame,\n",
    "        test_df: DataFrame,\n",
    "        num_few_shot: int,\n",
    "        max_length: int,\n",
    "        cot: bool # not used\n",
    "        ):\n",
    "    choice_ids = [tokenizer.convert_tokens_to_ids(choice)[0] for choice in CMMLU.mp_utils.choices]\n",
    "    cors = []\n",
    "    all_conf = [] # confidence\n",
    "    all_preds = [] # predicates\n",
    "    answers = CMMLU.mp_utils.choices[: test_df.shape[1] - 2]\n",
    "\n",
    "    batch_size = 20\n",
    "    bucket = []\n",
    "    def run_and_cleanup_bucket() -> None:\n",
    "        nonlocal bucket, all_preds, all_conf\n",
    "        if len(bucket) == 0:\n",
    "            return\n",
    "        prompts = [prompt for prompt, label in bucket]\n",
    "        labels = [label for prompt, label in bucket]\n",
    "        bucket = []\n",
    "        inputs = tokenizer(prompts).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_token_logits = outputs[:, -1, :].to(dtype=torch.float32) # shape: [bz, vocab_size]\n",
    "            choice_logits: torch.Tensor = last_token_logits[:, choice_ids]\n",
    "            index_of_labels = torch.tensor([choices.index(label) for label in labels], dtype=torch.int64)[:, None].to('cuda')\n",
    "            confidences = torch.nn.functional.softmax(choice_logits, dim=-1)\n",
    "            confidences = confidences.gather(dim=-1, index=index_of_labels).view(-1)\n",
    "            predictions = [choices[pred_index] for pred_index in choice_logits.argmax(dim=-1).tolist()]\n",
    "        all_preds.extend(predictions)\n",
    "        all_conf.extend(confidences.tolist())\n",
    "        cors.extend([pred==label for pred, label in zip(predictions, labels)])\n",
    "\n",
    "    for i in range(test_df.shape[0]):\n",
    "        prompt_end = format_example(test_df, i, subject, include_answer=False)\n",
    "        prompt = gen_prompt(dev_df=dev_df,\n",
    "                            subject=subject,\n",
    "                            prompt_end=prompt_end,\n",
    "                            num_few_shot=num_few_shot,\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_length=max_length)\n",
    "        label = test_df.iloc[i, test_df.shape[1] - 1]\n",
    "        bucket.append((prompt, label))\n",
    "\n",
    "        if len(bucket) >= batch_size:\n",
    "            run_and_cleanup_bucket()\n",
    "\n",
    "    run_and_cleanup_bucket()\n",
    "\n",
    "    acc = np.mean(cors)\n",
    "    print(\"Average accuracy {:.3f} - {}\".format(acc, subject))\n",
    "    return acc, all_preds, all_conf\n",
    "\n",
    "    # print(\"answers\", answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/NLP/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy 0.249 - agronomy\n",
      "Average accuracy 0.270 - anatomy\n",
      "Average accuracy 0.238 - ancient_chinese\n",
      "Average accuracy 0.250 - arts\n",
      "Average accuracy 0.261 - astronomy\n",
      "Average accuracy 0.287 - business_ethics\n",
      "Average accuracy 0.263 - chinese_civil_service_exam\n",
      "Average accuracy 0.237 - chinese_driving_rule\n",
      "Average accuracy 0.206 - chinese_food_culture\n",
      "Average accuracy 0.252 - chinese_foreign_policy\n",
      "Average accuracy 0.254 - chinese_history\n",
      "Average accuracy 0.250 - chinese_literature\n",
      "Average accuracy 0.257 - chinese_teacher_qualification\n",
      "Average accuracy 0.228 - clinical_knowledge\n",
      "Average accuracy 0.274 - college_actuarial_science\n",
      "Average accuracy 0.355 - college_education\n",
      "Average accuracy 0.321 - college_engineering_hydrology\n",
      "Average accuracy 0.231 - college_law\n",
      "Average accuracy 0.200 - college_mathematics\n",
      "Average accuracy 0.189 - college_medical_statistics\n",
      "Average accuracy 0.275 - college_medicine\n",
      "Average accuracy 0.265 - computer_science\n",
      "Average accuracy 0.240 - computer_security\n",
      "Average accuracy 0.259 - conceptual_physics\n",
      "Average accuracy 0.223 - construction_project_management\n",
      "Average accuracy 0.258 - economics\n",
      "Average accuracy 0.252 - education\n",
      "Average accuracy 0.262 - electrical_engineering\n",
      "Average accuracy 0.278 - elementary_chinese\n",
      "Average accuracy 0.242 - elementary_commonsense\n",
      "Average accuracy 0.265 - elementary_information_and_technology\n",
      "Average accuracy 0.235 - elementary_mathematics\n",
      "Average accuracy 0.289 - ethnology\n",
      "Average accuracy 0.252 - food_science\n",
      "Average accuracy 0.239 - genetics\n",
      "Average accuracy 0.268 - global_facts\n",
      "Average accuracy 0.254 - high_school_biology\n",
      "Average accuracy 0.303 - high_school_chemistry\n",
      "Average accuracy 0.237 - high_school_geography\n",
      "Average accuracy 0.244 - high_school_mathematics\n",
      "Average accuracy 0.236 - high_school_physics\n",
      "Average accuracy 0.245 - high_school_politics\n",
      "Average accuracy 0.198 - human_sexuality\n",
      "Average accuracy 0.227 - international_law\n",
      "Average accuracy 0.291 - journalism\n",
      "Average accuracy 0.253 - jurisprudence\n",
      "Average accuracy 0.276 - legal_and_moral_basis\n",
      "Average accuracy 0.333 - logical\n",
      "Average accuracy 0.295 - machine_learning\n",
      "Average accuracy 0.262 - management\n",
      "Average accuracy 0.278 - marketing\n",
      "Average accuracy 0.222 - marxist_theory\n",
      "Average accuracy 0.181 - modern_chinese\n",
      "Average accuracy 0.234 - nutrition\n",
      "Average accuracy 0.210 - philosophy\n",
      "Average accuracy 0.217 - professional_accounting\n",
      "Average accuracy 0.227 - professional_law\n",
      "Average accuracy 0.255 - professional_medicine\n",
      "Average accuracy 0.237 - professional_psychology\n",
      "Average accuracy 0.282 - public_relations\n",
      "Average accuracy 0.274 - security_study\n",
      "Average accuracy 0.248 - sociology\n",
      "Average accuracy 0.273 - sports_science\n",
      "Average accuracy 0.270 - traditional_chinese_medicine\n",
      "Average accuracy 0.254 - virology\n",
      "Average accuracy 0.248 - world_history\n",
      "Average accuracy 0.181 - world_religions\n",
      "STEM                                     25.64\n",
      "Humanities                               24.28\n",
      "Social Science                           25.84\n",
      "Other                                    24.78\n",
      "China specific                           24.56\n",
      "Overall                        25.25\n"
     ]
    }
   ],
   "source": [
    "import CMMLU.hf_causal_model\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_name_or_path\", type=str, default=\"\")\n",
    "parser.add_argument(\"--lora_weights\", type=str, default=\"\")\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"CMMLU/data\")\n",
    "parser.add_argument(\"--save_dir\", type=str, default=\"CMMLU/results/not_specified\")\n",
    "parser.add_argument(\"--num_few_shot\", type=int, default=0)\n",
    "parser.add_argument(\"--max_length\", type=int, default=2048)\n",
    "parser.add_argument(\"--load_in_8bit\", action='store_true')\n",
    "parser.add_argument(\"--with_conf\", action='store_true')\n",
    "parser.add_argument(\"--cot\", action='store_true')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "random.seed(233)\n",
    "\n",
    "CMMLU.mp_utils.run_eval(\n",
    "    model, cmmlu_tokenizer,\n",
    "    eval,\n",
    "    args\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
